{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project aims to develop a neural network model that predicts stress levels based on physiological signals from the WESAD dataset. Let's delve into each step with a more detailed explanation and an illustrative example:\n",
    "\n",
    "1. Dataset Exploration :\n",
    "\n",
    "Download the WESAD dataset. In the search bar of Google or any other browser, type \"ics.uci.edu\" datasets and search for WESAD dataset.\n",
    "\n",
    "Use libraries like Pandas (Python) to explore the data structure. Imagine the data is stored in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv(\"WESAD_dataset.csv\")\n",
    "\n",
    "# Print the first few rows to get a glimpse of the data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "The output of head() will show the first few rows of the data, revealing column names representing features like:\n",
    "\n",
    "Electrocardiogram (ECG) - Electrical activity of the heart\n",
    "\n",
    "Electrodermal Activity (EDA) - Skin conductance, often linked to sweat production\n",
    "\n",
    "Electromyography (EMG) - Electrical activity of muscles\n",
    "\n",
    "Respiration rate - Number of breaths per minute\n",
    "\n",
    "Label - Indicating stress level (e.g., \"Low\", \"Medium\", \"High\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Data Preprocessing :\n",
    "\n",
    "Handle missing values:\n",
    "\n",
    "Identify missing values using functions like data.isnull().sum().\n",
    "\n",
    "Choose an appropriate method to fill them. Common techniques include:\n",
    "\n",
    "2.1)Forward fill (ffill): Replace missing values with the value from the previous time point.\n",
    "\n",
    "2.2)Backward fill (bfill): Replace missing values with the value from the next time point.\n",
    "\n",
    "2.3)Interpolation: Estimate missing values based on surrounding data points.\n",
    "\n",
    "2.4)Deletion: Remove rows or columns with a high percentage of missing values (consider the impact on data size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Identifying missing values in ECG data\n",
    "missing_ecg_values = data[\"ECG\"].isnull().sum()\n",
    "print(f\"Number of missing ECG values: {missing_ecg_values}\")\n",
    "\n",
    "# Example: Filling missing ECG values with forward fill\n",
    "data[\"ECG\"] = data[\"ECG\"].fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Normalize the data:  In the following step , Standardize the features to have a mean of 0 and a standard deviation. This ensures all features contribute equally to the model's learning process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize all features except the label\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.drop(\"Label\", axis=1))\n",
    "\n",
    "# Combine the scaled features back with the label column\n",
    "data_preprocessed = pd.concat([pd.DataFrame(data_scaled), data[\"Label\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data: Now, Divide the preprocessed data into two sets: training and validation. The training set is used to train the model, while the validation set is used to evaluate its performance on unseen data. A common split is 80% for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_preprocessed.drop(\"Label\", axis=1), \n",
    "                                                    data_preprocessed[\"Label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture:\n",
    "\n",
    "\n",
    "\n",
    "Consider two potential architectures suitable for physiological signal processing:\n",
    "\n",
    "Convolutional Neural Network (CNN):\n",
    "\n",
    "Effective at extracting features from sequential data like ECG.\n",
    "\n",
    "Uses convolutional layers to identify local patterns within the signal.\n",
    "\n",
    "Followed by pooling layers for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense\n",
    "\n",
    "# Example CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())  # Flatten the output of the convolutional layers\n",
    "model.add(Dense(units=128, activation=\"relu\"))  # Hidden layer with ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Model: This line defines a model structure where layers are stacked sequentially.\n",
    "\n",
    "2. Conv1D Layer:\n",
    "\n",
    "This applies a convolutional filter to extract features from one-dimensional data (like a single physiological signal).\n",
    "\n",
    "It has 32 filters, a kernel size of 3, and uses ReLU activation for non-linearity.\n",
    "\n",
    "input_shape=(X_train.shape[1], 1) specifies the input shape:\n",
    "\n",
    "X_train.shape[1]: Number of time steps in the signal (assuming data is preprocessed).\n",
    "\n",
    "1: Number of features (assuming we're processing a single signal at a time).\n",
    "\n",
    "3. MaxPooling1D Layer:\n",
    "\n",
    "This down samples the output of the convolutional layer, reducing the number of parameters and computational cost.\n",
    "\n",
    "The pool size of 2 means it takes the maximum value within windows of size 2 and moves one step forward, effectively reducing the dimensionality.\n",
    "\n",
    "4. Flatten Layer:\n",
    "\n",
    "This transforms the two-dimensional output of the pooling layer (time steps, features) into a one-dimensional vector suitable for feeding into the Dense layer.\n",
    "\n",
    "5. Dense Layer:\n",
    "\n",
    "This is a fully connected layer with 128 units and ReLU activation. It further processes the extracted features from the convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs):\n",
    "Explanation:\n",
    "Recurrent Neural Networks (RNNs) are a powerful class of neural networks designed to handle sequential data like time series. Unlike traditional neural networks that process data points independently, RNNs can capture relationships and dependencies between data points at different points in time. \n",
    "\n",
    "RNNs achieve their ability to learn temporal dependencies through their internal loop structure. This loop allows them to process information sequentially and maintain a \"memory\" of past inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM layer for feature extraction\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(window_length, n_features)))\n",
    "\n",
    "# Stack additional LSTM layers (optional) for complex relationships\n",
    "# model.add(LSTM(units=32, return_sequences=True))\n",
    "\n",
    "# Flatten the output sequence\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer for further processing\n",
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "\n",
    "# Output layer (modify for binary or multi-class classification)\n",
    "model.add(Dense(n_classes, activation=\"softmax\"))  # n_classes = number of stress levels\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model on your preprocessed data\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# ... (Rest of your code for evaluation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Sequential Model: Defines a series of layers stacked one after another.\n",
    "\n",
    "LSTM Layer:\n",
    "\n",
    "This layer processes the preprocessed physiological signals one time step at a time (window of data).\n",
    "\n",
    "units=64: Adjust this hyperparameter based on data complexity and experimentation.\n",
    "\n",
    "return_sequences=True: Ensures the entire sequence of hidden states is returned, capturing temporal information.\n",
    "\n",
    "input_shape=(window_length, n_features):\n",
    "\n",
    "4.1)window_length: Length of the time window used for segmentation.\n",
    "\n",
    "4.2)n_features: Number of features extracted from each signal (e.g., mean, standard deviation).\n",
    "\n",
    "Optional Stacked LSTMs: You can add additional LSTM layers (commented out) to capture even more complex relationships within the data. However, experiment to avoid overfitting.\n",
    "\n",
    "Flatten Layer: Reshapes the output sequence from the LSTM layer into a one-dimensional vector suitable for the Dense layer.\n",
    "\n",
    "Dense Layer: Processes the extracted features further before feeding them into the output layer.\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "Adjust the number of neurons (n_classes) and activation function based on your stress classification task:\n",
    "\n",
    "Binary classification (stressed/not stressed): 1 neuron with sigmoid activation.\n",
    "\n",
    "Multi-class classification (low, medium, high stress): More neurons with softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation=\"sigmoid\"))  # Output layer for binary classification (stressed/not stressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])  # Adjust for multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 10 epochs with a batch size of 32 samples\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model  # Assuming you saved the trained model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"my_trained_model.h5\")  # Replace with your model filename\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "loss, accuracy = model.evaluate(X_val, y_val)  # X_val and y_val are validation data\n",
    "\n",
    "# Print basic evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1-score (using external library)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "y_pred = model.predict_classes(X_val)  # Get model predictions on validation data\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_val, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a function to build the model with tunable hyperparameters\n",
    "def build_model(learning_rate, num_lstm_units):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(units=num_lstm_units, return_sequences=True, input_shape=(window_length, n_features)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(units=128, activation=\"relu\"))\n",
    "  model.add(Dense(n_classes, activation=\"softmax\"))\n",
    "  model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
    "  return model\n",
    "\n",
    "# ... (Rest of your code for data preparation, etc.)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "learning_rates = [0.001, 0.0001]\n",
    "num_lstm_units = [32, 64, 128]\n",
    "\n",
    "# Grid search for hyperparameter tuning (replace with other search methods)\n",
    "for lr in learning_rates:\n",
    "  for units in num_lstm_units:\n",
    "    model = build_model(lr, units)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "    # Evaluate and record performance for each hyperparameter combination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
